model: meta-llama/Meta-Llama-3-70B-Instruct
backend: vllm
vllm_server_host: 0.0.0.0
vllm_server_port: 8000

generation_kwargs:
  max_completion_tokens: 256
  temperature: 0.7
  top_p: 0.8
